---
layout: post
title: MinIO Object Storage
tags: [OpenStack]
category: FINALIZED
color: rgb(250, 154, 133)
feature-img: "assets/img/post-cover/21-cover.png"
thumbnail: "assets/img/post-cover/21-cover.png"
author: QubitPi
excerpt_separator: <!--more-->
---

<!--more-->

* TOC
{:toc}

## Docker on Mac (and only on Mac)

https://stackoverflow.com/questions/67965464/i-keep-getting-this-error-when-trying-to-install-anything-on-mac-big-sure

### Prerequisite

    brew install python@3.9
    brew install netpbm
    brew install libssh
    brew install podman
    podman machine init
    podman machine start

* Image: https://hub.docker.com/r/minio/minio/
* Run container using [padman](https://podman.io/getting-started/installation)


## What Is Object Storage?

An object is binary data, sometimes referred to as a Binary Large OBject (BLOB). Blobs can be

* images
* audio files 
* spreadsheets
* binary executable code
  
Object Storage platforms like MinIO provide dedicated tools and capabilities for storing, retrieving, and searching for
blobs.

MinIO Object Storage uses buckets to organize objects. A bucket is similar to a folder or directory in a filesystem,
where each bucket can hold an arbitrary number of objects. **MinIO buckets provide the same functionality as AWS S3
buckets**.

For example, consider an application that hosts a web blog. The application needs to store a variety of blobs, including
rich multimedia like videos and images. The structure of objects on the MinIO server might look similar to the
following:

```
/ #root
/images/
   2020-01-02-MinIO-Diagram.png
   2020-01-03-MinIO-Advanced-Deployment.png
   MinIO-Logo.png
/videos/
   2020-01-04-MinIO-Interview.mp4
/articles/
   /john.doe/
      2020-01-02-MinIO-Object-Storage.md
      2020-01-02-MinIO-Object-Storage-comments.json
   /jane.doe/
      2020-01-03-MinIO-Advanced-Deployment.png
      2020-01-02-MinIO-Advanced-Deployment-comments.json
      2020-01-04-MinIO-Interview.md
```

MinIO supports multiple levels of nested directories and objects to support even the most dynamic object storage
workloads.

## Erasure Coding

MinIO Erasure Coding is a data redundancy and availability feature that allows MinIO deployments to automatically
reconstruct objects on-the-fly despite the loss of multiple drives or nodes in the cluster. Erasure Coding provides
object-level healing with less overhead than adjacent technologies such as RAID or replication.

Erasure Coding splits objects into data and parity blocks, where parity blocks support reconstruction of missing or
corrupted data blocks. MinIO distributes both data and parity blocks across
[minio server](https://docs.min.io/minio/baremetal/reference/minio-server/minio-server.html#command-minio-server) nodes
and drives in an [Erasure Set](#erasure-sets). Depending on the configured parity, number of nodes, and number of drives
per node in the Erasure Set, MinIO can tolerate the loss of up to half (N/2) of drives and still retrieve stored
objects.

For example, consider a small-scale MinIO deployment consisting of a single Server Pool with 4 minio server nodes. Each
node in the deployment has 4 locally attached 1Ti drives for a total of 16 drives.

MinIO creates [Erasure Set](#erasure-sets) by dividing the total number of drives in the deployment into sets consisting
of between 4 and 16 drives each. In the example deployment, the largest possible Erasure Set size that evenly divides
into the total number of drives is 16.

MinIO uses a [Reed-Solomon algorithm](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction) to split
objects into data and parity blocks based on the size of the Erasure Set. MinIO then uniformly distributes the data and
parity blocks across the Erasure Set drives such that each drive in the set contains no more than one block per object.
MinIO uses the "EC:N" notation to refer to the number of parity blocks (N) in the Erasure Set.

**The number of parity blocks in a deployment controls the deployment's relative data redundancy**. Higher levels of
parity allow for higher tolerance of drive loss at the cost of total available storage. For example, using EC:4 in our
example deployment results in 12 data blocks and 4 parity blocks. The parity blocks take up some portion of space in the
deployment, reducing total storage. However, the parity blocks allow MinIO to reconstruct the object with only 8 data
blocks, increasing resilience to data corruption or loss.

### Erasure Sets

An _Erasure Set_ is a set of drives in a MinIO deployment that support Erasure Coding. MinIO evenly distributes object
data and parity blocks among the drives in the Erasure Set.

MinIO calculates the number and size of Erasure Sets by dividing the total number of drives in the Server Pool into sets
consisting of between 4 and 16 drives each. MinIO considers two factors when selecting the Erasure Set size:

1. The Greatest Common Divisor (GCD) of the total drives.
2. The number of
   [minio server](https://docs.min.io/minio/baremetal/reference/minio-server/minio-server.html#command-minio-server)
   nodes in the Server Pool.

For an even number of nodes, MinIO uses the GCD to calculate the Erasure Set size and ensure the minimum number of
Erasure Sets possible. For an odd number of nodes, MinIO selects a common denominator that results in an odd number of
Erasure Sets to facilitate more uniform distribution of erasure set drives among nodes in the Server Pool.

For example, consider a Server Pool consisting of 4 nodes with 8 drives each for a total of 32 drives. The GCD of 16
produces 2 Erasure Sets of 16 drives each with uniform distribution of erasure set drives across all 4 nodes.

Now consider a Server Pool consisting of 5 nodes with 8 drives each for a total of 40 drives. Using the GCD, MinIO would
create 4 erasure sets with 10 drives each. However, this distribution would result in uneven distribution with one node
contributing more drives to the Erasure Sets than the others. MinIO instead creates 5 erasure sets with 8 drives each to
ensure uniform distribution of Erasure Set drives per Nodes.

**MinIO generally recommends maintaining an even number of nodes in a Server Pool** to facilitate simplified human
calculation of the number and size of Erasure Sets in the Server Pool.

### Erasure Code Parity (EC:N)

MinIO uses a [Reed-Solomon algorithm](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction) to split
objects into data and parity blocks based on the size of the Erasure Set. MinIO uses parity blocks to automatically heal
damaged or missing data blocks when reconstructing an object. MinIO uses the EC:N notation to refer to the number of
parity blocks (N) in the Erasure Set.

**MinIO uses a hash of an object's name to determine into which Erasure Set to store that object**. MinIO always uses
that erasure set for objects with a matching name. For example, MinIO stores all versions of an object in the same
Erasure Set.

After MinIO selects an object's Erasure Set, it divides the object based on the number of drives in the set and the
configured parity. MinIO creates:

* "(Erasure Set Drives) - EC:N' number of Data Blocks, and
* "EC:N" number of Parity Blocks.

MinIO randomly and uniformly distributes the data and parity blocks across drives in the erasure set with no overlap.
While a drive may contain both data and parity blocks for multiple unique objects, a single unique object has no more
than one block per drive in the set. For versioned objects, MinIO selects the same drives for both data and parity
storage while maintaining zero overlap on any single drive.

The specified parity for an object also dictates the minimum number of Erasure Set drives ("Quorum") required for MinIO
to either read or write that object:

* **Read Quorum** - The minimum number of Erasure Set drives required for MinIO to serve read operations. MinIO can
  automatically reconstruct an object with corrupted or missing data blocks if enough drives are online to provide Read
  Quorum for that object.

  MinIO Read Quorum is "DRIVES - (EC:N)".
  
* **Write Quorum** - The minimum number of Erasure Set drives required for MinIO to serve write operations. MinIO
  requires enough available drives to eliminate the risk of split-brain scenarios.

  MinIO Write Quorum is "(DRIVES - (EC:N)) + 1".

#### Storage Classes

MinIO supports storage classes with Erasure Coding to allow applications to specify per-object parity. Each storage
class specifies a EC:N parity setting to apply to objects created with that class.

MinIO provides the following two storage classes:

1. `STANDARD` - The STANDARD storage class is the default class for all objects. You can configure the `STANDARD`
   storage class parity using either:

    - The `MINIO_STORAGE_CLASS_STANDARD` environment variable, or
    - The `mc admin config` command to modify the `storage_class.standard` configuration setting.

2. `REDUCED_REDUNDANCY` - The `REDUCED_REDUNDANCY` storage class allows creating objects with lower parity than
   `STANDARD`. You can configure the `REDUCED_REDUNDANCY` storage class parity using either:

   - The `MINIO_STORAGE_CLASS_RRS` environment variable, or
   - The `mc admin config` command to modify the `storage_class.rrs` configuration setting.

`REDUCED_REDUNDANCY` is not supported for MinIO deployments with 4 or fewer drives.

MinIO references the `x-amz-storage-class` header in request metadata for determining which storage class to assign an
object. 

### BitRot Protection

Silent data corruption or bitrot is a serious problem faced by disk drives resulting in data getting corrupted without
the user's knowledge. The reasons are manifold (ageing drives, current spikes, bugs in disk firmware, phantom writes,
misdirected reads/writes, driver errors, accidental overwrites) but the result is the same - compromised data.

MinIO's optimized implementation of the HighwayHash algorithm ensures that it will never read corrupted data - it
captures and heals corrupted objects on the fly. Integrity is ensured from end to end by computing a hash on READ and
verifying it on WRITE from the application, across the network and to the memory/drive. The implementation is designed
for speed and can achieve hashing speeds over 10 GB/sec on a single core on Intel CPUs.
