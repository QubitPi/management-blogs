---
layout: post
title: Rancher - A Virtualized CI/CD on top of Kubernetes 
tags: [Virtualization]
color: rgb(224, 1, 152)
feature-img: "assets/img/post-cover/36-cover.png"
thumbnail: "assets/img/post-cover/36-cover.png"
author: QubitPi
excerpt_separator: <!--more-->
---

<!--more-->

* TOC
{:toc}

## Overview

Rancher is a container management platform built for organizations that deploy containers in production.

Rancher users have the choice of creating Kubernetes clusters with Rancher Kubernetes Engine (RKE) or cloud Kubernetes
services, such as GKE, AKS, and EKS.

### Meet IT Requirements

Rancher supports centralized authentication, access control, and monitoring for all Kubernetes clusters under its
control. For example, you can:

* Use your Active Directory credentials to access Kubernetes clusters hosted by cloud vendors, such as GKE.
* Setup and enforce access control and security policies across all users, groups, projects, clusters, and clouds.
* View the health and capacity of your Kubernetes clusters from a single-pane-of-glass.

### Empower DevOps Teams

Rancher provides an intuitive user interface for DevOps engineers to manage their application workload. Rancher is
certified with a wide selection of cloud native ecosystem products, including, for example, security tools, monitoring
systems, container registries, and storage and networking drivers.

The following figure illustrates the role Rancher plays in IT and DevOps organizations. _Each team deploys their
applications on the public or private clouds they choose. IT administrators gain visibility and enforce policies across
all users, clusters, and clouds.

![rancher-platform.png not loaded property]({{ "/assets/img/rancher-platform.png" | relative_url}})

### Rancher API Server

The Rancher API server is built on top of an embedded Kubernetes API server and an [etcd database](https://etcd.io/). It
implements the following functionalities:

#### Authorization and Role-Based Access Control

* **User management**: The Rancher API server
  [manages user identities](https://rancher.com/docs/rancher/v2.5/en/admin-settings/authentication/) that correspond to
  external authentication providers like Active Directory or GitHub, in addition to local users.
* **Authorization**: The Rancher API server manages
  [access control](https://rancher.com/docs/rancher/v2.5/en/admin-settings/rbac/) and
  [security](https://rancher.com/docs/rancher/v2.5/en/admin-settings/pod-security-policies/) policies.
  
#### Working with Kubernetes

* **Provisioning Kubernetes clusters**: The Rancher API server can
  [provision Kubernetes](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/) on existing nodes, or perform
  [Kubernetes upgrades](https://rancher.com/docs/rancher/v2.5/en/cluster-admin/upgrading-kubernetes).
* **Catalog management**: Rancher provides the ability to use a
  [catalog of Helm charts](https://rancher.com/docs/rancher/v2.5/en/catalog/) that make it easy to repeatedly deploy
  applications.
* **Managing projects**: **A project is a group of multiple namespaces and access control policies within a cluster**. A
  project is a Rancher concept, not a Kubernetes concept, which allows you to manage multiple namespaces as a group and
  perform Kubernetes operations in them. The Rancher UI provides features for
  [project administration](https://rancher.com/docs/rancher/v2.5/en/project-admin/) and
  [for managing applications within projects](https://rancher.com/docs/rancher/v2.5/en/k8s-in-rancher/).
* **Pipelines**: Setting up a [pipeline](https://rancher.com/docs/rancher/v2.5/en/project-admin/pipelines/) can help
  developers deliver new software as quickly and efficiently as possible. Within Rancher, you can configure pipelines
  for each of your Rancher projects.
* **Istio**: Our integration with Istio is designed so that a Rancher operator, such as an administrator or cluster
  owner, can deliver Istio to developers. Then developers can use Istio to enforce security policies, troubleshoot
  problems, or manage traffic for green/blue deployments, canary deployments, or A/B testing.

#### Working with Cloud Infrastructure

* **Tracking nodes**: The Rancher API server tracks identities of all the nodes in all clusters.
* **Setting up infrastructure**: When configured to use a cloud provider, Rancher can dynamically provision new nodes
  and persistent storage in the cloud.

#### Cluster Visibility

* **Logging**: Rancher can integrate with a variety of popular logging services and tools that exist outside of your
  Kubernetes clusters.
* **Monitoring**: Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components,
  and software deployments through integration with Prometheus, a leading open-source monitoring solution.
* **Alerting**: To keep your clusters and applications healthy and driving your organizational productivity forward, you
  need to stay informed of events occurring in your clusters and projects, both planned and unplanned.

## Architecture

The figure depicts a Rancher Server installation that manages two downstream Kubernetes clusters: one created by
[RKE](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/) and  another created by
[Amazon EKS (Elastic Kubernetes Service)](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/hosted-kubernetes-clusters/).

![rancher-architecture-rancher-api-server.svg not loaded property]({{ "/assets/img/rancher-architecture-rancher-api-server.svg" | relative_url}})

### Communicating with Downstream User Clusters

The below diagram shows how the cluster controllers, cluster agents, and node agents allow Rancher to control downstream
clusters.

![rancher-architecture-cluster-controller.svg not loaded property]({{ "/assets/img/rancher-architecture-cluster-controller.svg" | relative_url}})

#### The Authentication Proxy

In this diagram, a user named Bob wants to see all pods running on a downstream user cluster called User Cluster 1.
From within Rancher, he can run a `kubectl` command to see the pods. Bob is authenticated through Rancher's
authentication proxy.

The authentication proxy forwards all Kubernetes API calls to downstream clusters. It integrates with authentication
services like local authentication, Active Directory, and GitHub. On every Kubernetes API call, the authentication proxy
authenticates the caller and sets the proper Kubernetes impersonation headers before forwarding the call to Kubernetes
masters.

Rancher communicates with Kubernetes clusters using a
[service account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/), which provides
an identity for processes that run in a pod.

#### Cluster Controllers and Cluster Agent

Each downstream user cluster has a cluster agent, which opens a tunnel to the corresponding cluster controller within
the Rancher server.

There is one cluster controller and one cluster agent for each downstream cluster. Each cluster controller:

* Watches for resource changes in the downstream cluster
* Brings the current state of the downstream cluster to the desired state
* Configures access control policies to clusters and projects
* Provisions clusters by calling the required Docker machine drivers and Kubernetes engines, such as RKE and GKE

By default, to enable Rancher to communicate with a downstream cluster, the cluster controller connects to the cluster
agent. If the cluster agent is not available, the cluster controller can connect to a [node agent](#node-agents)
instead.

#### Node Agents

If the cluster agent (also called `cattle-cluster-agent`) is not available, one of the node agents creates a tunnel to
the cluster controller to communicate with Rancher.

The `cattle-node-agent` is deployed using a
[DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) resource to make sure it runs on every
node in a Rancher-launched Kubernetes cluster. It is used to interact with the nodes when performing cluster operations.
Examples of cluster operations include upgrading the Kubernetes version and creating or restoring etcd snapshots.

#### Authorized Cluster Endpoint

An authorized cluster endpoint allows users to connect to the Kubernetes API server of a downstream cluster without
having to route their requests through the Rancher authentication proxy.

> The authorized cluster endpoint only works on Rancher-launched Kubernetes clusters. In other words, it only works in
> clusters where Rancher [used RKE](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters) to
> provision the cluster. It is not available for registered clusters, or for clusters in a hosted Kubernetes provider,
> such as Amazon EKS.

There are two main reasons why a user might need the authorized cluster endpoint:

1. To access a downstream user cluster while Rancher is down
2. To reduce latency in situations where the Rancher server and downstream cluster are separated by a long distance

## Pipelines

Rancher pipeline is very similar to [Yahoo's Screwdriver](https://screwdriver.cd/), which runs based on a YAML config
file, which contains all deployment specs, in git project and the file gets pushed to Jenkins to be executed.

Rancher's pipeline provides a simple CI/CD experience. Use it to **automatically**

* checkout code,
* run builds or scripts, 
* publish Docker images, and
* deploy the updated software to users

Setting up a pipeline can help developers deliver new software as quickly and efficiently as possible. Using Rancher,
you can integrate with a GitHub repository to setup a continuous integration (CI) pipeline.

After configuring Rancher and GitHub, you can deploy containers running Jenkins to automate a pipeline execution:

* Build your application from code to image.
* Validate your builds.
* Deploy your build images to your cluster.
* Run unit tests.
* Run regression tests.

### Concepts

* **Pipeline**: A _pipeline_ is a software delivery process that is broken into different stages and steps. Setting up a
  pipeline can help developers deliver new software as quickly and efficiently as possible. Within Rancher, you can
  configure pipelines for each of your Rancher projects. **A pipeline is based on a specific repository**. It defines
  the process to build, test, and deploy your code. Rancher uses the
  [pipeline as code](https://www.jenkins.io/doc/book/pipeline-as-code/) model. **Pipeline configuration is represented
  as a pipeline file in the source code repository**, using the file name `.rancher-pipeline.yml` or
  `.rancher-pipeline.yaml`.
* **Stages**: A pipeline stage consists of multiple steps. Stages are executed in the order defined in the pipeline
  file. The steps in a stage are executed concurrently. A stage starts when all steps in the former stage finish without
  failure.
* **Steps**: A pipeline step is executed inside a specified stage. A step fails if it exits with a code other than 0. If
  a step exits with this failure code, the entire pipeline fails and terminates.
* **Workspace**: The workspace is the working directory shared by all pipeline steps. In the beginning of a pipeline,
  source code is checked out to the workspace. The command for every step bootstraps in the workspace. During a pipeline
  execution, the artifacts from a previous step will be available in future steps. The working directory is an ephemeral
  volume and will be cleaned out with the executor pod when a pipeline execution is finished.

Typically, pipeline stages include:

1. **Build**: Each time code is checked into your repository, the pipeline automatically clones the repo and builds a
   new iteration of your software. Throughout this process, the software is typically reviewed by automated tests.
2. **Publish**: After the build is completed, either a Docker image is built and published to a Docker registry
3. **Deploy**: After the artifacts are published, you would release your application so users could start using the
   updated product.

### How Pipelines Work

After enabling the ability to use pipelines in a project, you can configure multiple pipelines in each project. Each
pipeline is unique and can be configured independently.

A pipeline is configured off of a group of files that are checked into source code repositories. Users can configure
their pipelines either through the Rancher UI or by adding a `.rancher-pipeline.yml` into the repository.

Before pipelines can be configured, you will need to configure authentication to your version control provider, e.g.
GitHub, GitLab, Bitbucket. If you haven't configured a version control provider, you can always use
[Rancher's example repositories](#example-repositories) to view some common pipeline deployments.

When you configure a pipeline in one of your projects, a namespace specifically for the pipeline is automatically
created. The following components are deployed to it:

1. **Jenkins** - The pipeline's build engine. Because project users do not directly interact with Jenkins, it's managed
   and locked.
   
   > Note that there is no option to use existing Jenkins deployments as the pipeline engine.

2. **Docker Registry** - Out-of-the-box, the default target for your build-publish step is an internal Docker Registry.
   However, _you can make configurations to push to a remote registry instead_. The internal Docker Registry is only
   accessible from cluster nodes and cannot be directly accessed by users. Images are not persisted beyond the lifetime
   of the pipeline and should only be used in pipeline runs. If you need to access your images outside of pipeline runs,
   please push to an external registry.
3. **Minio** - Minio storage is used to store the logs for pipeline executions.

> The managed Jenkins instance works statelessly, so don't worry about its data persistency. The Docker Registry and
> Minio instances use ephemeral volumes by default, which is fine for most use cases. If you want to make sure pipeline
> logs survive node failures, you can
> [configure persistent volumes for them](https://rancher.com/docs/rancher/v2.5/en/pipelines/storage/)

## Helm Charts in Rancher

### Charts

[Helm](https://helm.sh/) uses a packaging format called _charts_. **A chart is a collection of files that describe a
related set of Kubernetes resources**. A single chart might be used to deploy something simple, like a memcached pod, or
something complex, like a full web app stack with HTTP servers, databases, caches, and so on.

**Charts are created as files laid out in a particular directory tree**. They can be packaged into versioned archives to
be deployed.

If you want to download and look at the files for a published chart, without installing it, you can do so with

`helm pull chartrepo/chartname`

#### The Chart File Structure

A chart is organized as a collection of files inside of a directory. **The directory name is the name of the chart**
(_without versioning information_). Thus, a chart describing WordPress would be stored in a `wordpress/` directory.

Inside of this directory, Helm will expect a structure that matches this:

```
wordpress/
  Chart.yaml          # A YAML file containing information about the chart
  LICENSE             # OPTIONAL: A plain text file containing the license for the chart
  README.md           # OPTIONAL: A human-readable README file
  values.yaml         # The default configuration values for this chart
  values.schema.json  # OPTIONAL: A JSON Schema for imposing a structure on the values.yaml file
  charts/             # A directory containing any charts upon which this chart depends.
  crds/               # Custom Resource Definitions
  templates/          # A directory of templates that, when combined with values,
                      # will generate valid Kubernetes manifest files.
  templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes
```

Helm reserves use of the `charts/`, `crds/`, and `templates/` directories, and of the listed file names. Other files
will be left as they are.

#### The Chart.yaml File

The `Chart.yaml` file is required for a chart. It contains the following fields:

```yaml
apiVersion: The chart API version (required)
name: The name of the chart (required)
version: A SemVer 2 version (required)
kubeVersion: A SemVer range of compatible Kubernetes versions (optional)
description: A single-sentence description of this project (optional)
type: The type of the chart (optional)
keywords:
    - A list of keywords about this project (optional)
home: The URL of this projects home page (optional)
sources:
    - A list of URLs to source code for this project (optional)
dependencies: # A list of the chart requirements (optional)
    - name: The name of the chart (nginx)
      version: The version of the chart ("1.2.3")
      repository: (optional) The repository URL ("https://example.com/charts") or alias ("@repo-name")
      condition: (optional) A yaml path that resolves to a boolean, used for enabling/disabling charts (e.g. subchart1.enabled )
      tags: # (optional)
          - Tags can be used to group charts for enabling/disabling together
      import-values: # (optional)
          - ImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items.
      alias: (optional) Alias to be used for the chart. Useful when you have to add the same chart multiple times
maintainers: # (optional)
    - name: The maintainers name (required for each maintainer)
      email: The maintainers email (optional for each maintainer)
      url: A URL for the maintainer (optional for each maintainer)
icon: A URL to an SVG or PNG image to be used as an icon (optional).
appVersion: The version of the app that this contains (optional). Needn't be SemVer. Quotes recommended.
deprecated: Whether this chart is deprecated (optional, boolean)
annotations:
    example: A list of annotations keyed by name (optional).
```

As of `v3.3.2`, additional fields are not allowed. The recommended approach is to add custom metadata in `annotations`.

##### Charts and Versioning

Every chart must have a version number. A version must follow the
[SemVer 2 standard](https://semver.org/spec/v2.0.0.html). Unlike Helm Classic, Helm v2 and later uses version numbers as
release markers. Packages in repositories are identified by name plus version.

For example, an `nginx` chart whose version field is set to version: `1.2.3` will be named "nginx-1.2.3.tgz"

More complex SemVer 2 names are also supported, such as `version: 1.2.3-alpha.1+ef365`. But non-SemVer names are
explicitly disallowed by the system.

The `version` field inside of the `Chart.yaml` is used by many of the Helm tools, including the CLI. When generating a
package, the helm package command will use the version that it finds in the `Chart.yaml` as a token in the package name.
The system assumes that the version number in the chart package name matches the version number in the `Chart.yaml`.
Failure to meet this assumption will cause an error.

##### The "apiVersion" Field

The `apiVersion` field should be "v2" for Helm charts that require at least Helm 3. Charts supporting previous Helm
versions have an apiVersion set to "v1" and are still installable by Helm 3.

##### The "kubeVersion" Field

The optional `kubeVersion` field can define semver constraints on supported Kubernetes versions. Helm will validate the
version constraints when installing the chart and fail if the cluster runs an unsupported Kubernetes version.

Version constraints may comprise space separated AND comparisons such as

```
>= 1.13.0 < 1.15.0
```

which themselves can be combined with the OR `||` operator like in the following example

```bash
>= 1.13.0 < 1.14.0 || >= 1.14.1 < 1.15.0
```

In this example the version `1.14.0` is excluded, which can make sense if a bug in certain versions is known to prevent
the chart from running properly.

Apart from version constrains employing operators `=` `!=` `>` `<` `>=` `<=` the following shorthand notations are
supported

* hyphen ranges for closed intervals, where `1.1 - 2.3.4` is equivalent to `>= 1.1 <= 2.3.4`.
* wildcards `x`, `X` and `*`, where `1.2.x` is equivalent to `>= 1.2.0 < 1.3.0`.
* tilde ranges (patch version changes allowed), where `~1.2.3` is equivalent to `>= 1.2.3 < 1.3.0`
* caret ranges (minor version changes allowed), where `^1.2.3` is equivalent to `>= 1.2.3 < 2.0.0`.

For a detailed explanation of supported semver constraints see
[Masterminds/semver](https://github.com/Masterminds/semver).

##### Deprecating Charts

When managing charts in a Chart Repository, it is sometimes necessary to deprecate a chart. The optional `deprecated`
field in `Chart.yaml` can be used to mark a chart as deprecated. If the latest version of a chart in the repository is
marked as deprecated, then the chart as a whole is considered to be deprecated. The chart name can be later reused by
publishing a newer version that is not marked as deprecated.

##### Chart Types

The `type` field defines the type of chart. There are two types

1. application and
2. library
   
Application is the default type and it is the standard chart which can be operated on fully. The
[library chart](https://helm.sh/docs/topics/library_charts/) provides  utilities or functions for the chart builder. A
library chart differs from an application chart because it is not installable and usually doesn't contain any resource
objects.

#### README and NOTES

A README for a chart should be formatted in Markdown (README.md), and should generally contain:

* A description of the application or service the chart provides
* Any prerequisites or requirements to run the chart
* Descriptions of options in `values.yaml` and default values
* Any other information that may be relevant to the installation or configuration of the chart

When hubs and other user interfaces display details about a chart that detail is pulled from the content in the
README.md file.

The chart can also contain a short plain text `templates/NOTES.txt` file that will be printed out after installation,
and when viewing the status of a release. This file is evaluated as a [template](#templates-and-values), and can be used
to display usage notes, next steps, or any other information relevant to a release of the chart. For example,
instructions could be provided for connecting to a database, or accessing a web UI. Since this file is printed to STDOUT
when running `helm install` or `helm status`, it is recommended to keep the content brief and point to the README for
greater detail.

#### Chart Dependencies

In Helm, one chart may depend on any number of other charts. These dependencies can be dynamically linked using the
`dependencies` field in `Chart.yaml` or brought in to the `charts/` directory and managed manually.

##### Managing Dependencies with the "dependencies" Field

The charts required by the current chart are defined as a list in the `dependencies` field.

```yaml
dependencies:
    - name: apache
      version: 1.2.3
      repository: https://example.com/charts
    - name: mysql
      version: 3.2.1
      repository: https://another.example.com/charts
```

* The `name` field is the name of the chart you want.
* The `version` field is the version of the chart you want.
* The `repository` field is the full URL to the chart repository. Note that you must also use `helm repo add` to add
  that repo locally.
* You might use the name of the repo instead of URL

```bash
$ helm repo add fantastic-charts https://fantastic-charts.storage.googleapis.com
```

```yaml
dependencies:
    - name: awesomeness
      version: 1.0.0
      repository: "@fantastic-charts"
```

Once you have defined dependencies, you can run `helm dependency update` and it will use your dependency file to
download all the specified charts into your `charts/` directory for you. For the example above, one would expect to see
the following files in the charts directory:

```
charts/
  apache-1.2.3.tgz
  mysql-3.2.1.tgz
```

##### Alias Field in Dependencies

In addition to the other fields above, each requirements entry may contain the optional field `alias`.

Adding an alias for a dependency chart would put a chart in dependencies using alias as name of new dependency.

One can use alias in cases where they need to access a chart with other name(s).

```yaml
# parentchart/Chart.yaml

dependencies:
    - name: subchart
      repository: http://localhost:10191
      version: 0.1.0
      alias: new-subchart-1
    - name: subchart
      repository: http://localhost:10191
      version: 0.1.0
      alias: new-subchart-2
    - name: subchart
      repository: http://localhost:10191
      version: 0.1.0
```

In the above example we will get 3 dependencies in all for parentchart:

```
subchart
new-subchart-1
new-subchart-2
```

The manual way of achieving this is by copy/pasting the same chart in the `charts/` directory multiple times with
different names.

##### Tags and Condition Fields in dependencies

In addition to the other fields above, each requirements entry may contain the optional fields `tags` and `condition`.

All charts are loaded by default. If `tags` or `condition` fields are present, they will be evaluated and used to
control loading for the chart(s) they are applied to.

* **Condition** - The condition field holds one or more YAML paths (delimited by commas). If this path exists in the top
  parent's values and resolves to a boolean value, the chart will be enabled or disabled based on that boolean value.
  Only the first valid path found in the list is evaluated and if no paths exist then the condition has no effect.
* **Tags** - The tags field is a YAML list of labels to associate with this chart. In the top parent's values, all
  charts with tags can be enabled or disabled by specifying the tag and a boolean value.
  
```yaml
# parentchart/Chart.yaml

dependencies:
    - name: subchart1
      repository: http://localhost:10191
      version: 0.1.0
      condition: subchart1.enabled, global.subchart1.enabled
      tags:
          - front-end
          - subchart1
    - name: subchart2
      repository: http://localhost:10191
      version: 0.1.0
      condition: subchart2.enabled,global.subchart2.enabled
      tags:
          - back-end
          - subchart2
```

```yaml
# parentchart/values.yaml

subchart1:
    enabled: true
tags:
    front-end: false
    back-end: true
```

In the above example all charts with the tag `front-end` would be disabled but since the `subchart1.enabled` path
evaluates to `true` in the parent's values, the condition will override the front-end tag and `subchart1` will be
enabled.

Since `subchart2` is tagged with `back-end` and that tag evaluates to `true`, `subchart2` will be enabled. Also note
that although `subchart2` has a condition specified, there is no corresponding path and value in the parent's values so
that condition has no effect.

##### Using the CLI with Tags and Conditions

The `--set` parameter can be used as usual to alter tag and condition values.

```
helm install --set tags.front-end=true --set subchart2.enabled=false
```

##### Tags and Condition Resolution

* Conditions (when set in values) always override tags. The first condition path that exists wins and subsequent ones
  for that chart are ignored.
* Tags are evaluated as 'if any of the chart's tags are true then enable the chart'.
* Tags and conditions values must be set in the top parent's values.
* The `tags:` key in values must be a top level key. Globals and nested `tags:` tables are not currently supported.

##### Importing Child Values via Dependencies

In some cases it is desirable to allow a child chart's values to propagate to the parent chart and be shared as common
defaults. An additional benefit of using the exports format is that it will enable future tooling to introspect
user-settable values.

The keys containing the values to be imported can be specified in the parent chart's `dependencies` in the field
`import-values` using a YAML list. Each item in the list is a key which is imported from the child chart's `exports`
field.

If a child chart's `values.yaml` file contains an exports field at the root, its contents may be imported directly into
the parent's values by specifying the keys to import as in the example below:

```yaml
# parent's Chart.yaml file

dependencies:
    - name: subchart
      repository: http://localhost:10191
      version: 0.1.0
      import-values:
          - data
```

```yaml
# child's values.yaml file

exports:
    data:
        myint: 99
```

The final parent values would contain our exported field:

```yaml
# parent's values

myint: 99
```

To access values that are not contained in the `exports` key of the child chart's values, you will need to specify the
source key of the values to be imported (`child`) and the destination path in the parent chart's values (`parent`).

The `import-values` in the example below instructs Helm to take any values found at `child:` path and copy them to the
parent's values at the path specified in `parent`:

```yaml
# parent's Chart.yaml file

dependencies:
    - name: subchart1
      repository: http://localhost:10191
      version: 0.1.0
      ...
      import-values:
          - child: default.data
            parent: myimports
```

In the example above, values found at `default.data` in the subchart1's values will be imported to the `myimports` key
in the parent chart's values initialized as follows:

```yaml
# parent's values.yaml file

myimports:
    myint: 0
    mybool: false
    mystring: "helm rocks!"
```

```yaml
# subchart1's values.yaml file

default:
    data:
        myint: 999
        mybool: true
```

The parent chart's resulting values would be:

```yaml
# parent's final values

myimports:
    myint: 999
    mybool: true
    mystring: "helm rocks!"
```

The parent's final values now contains the `myint` and `mybool` fields imported from `subchart1`

##### Managing Dependencies Manually through the charts/ Directory

If more control over dependencies is desired, these dependencies can be expressed explicitly by copying the dependency
charts into the `charts/` directory.

A dependency should be an unpacked chart directory but its name cannot start with `_` or `.`. Such files are ignored by
the chart loader.

For example, if the WordPress chart depends on the Apache chart, the Apache chart (of the correct version) is supplied
in the WordPress chart's `charts/` directory:

```yaml
wordpress:
    Chart.yaml
    # ...
    charts/
        apache/
            Chart.yaml
            # ...
        mysql/
            Chart.yaml
            # ...
```

The example above shows how the WordPress chart expresses its dependency on Apache and MySQL by including those charts
inside of its `charts/` directory.

> ðŸ’¡ To drop a dependency into your charts/ directory, use the helm pull command

##### Operational Aspects of using Dependencies

How does chart dependencies affect chart installation using `helm install` and `helm upgrade`?

Suppose that a chart named "A" creates the following Kubernetes objects

* namespace "A-Namespace"
* statefulset "A-StatefulSet"
* service "A-Service"

Furthermore, A is dependent on chart B that creates objects

* namespace "B-Namespace"
* replicaset "B-ReplicaSet"
* service "B-Service"


After installation/upgrade of chart A a single Helm release is created/modified. The release will create/update all of
the above Kubernetes objects in the following order:

* A-Namespace
* B-Namespace
* A-Service
* B-Service
* B-ReplicaSet
* A-StatefulSet

This is because when Helm installs/upgrades charts, the Kubernetes objects from the charts and all its dependencies are

* aggregated into a single set; then
* sorted by type followed by name; and then
* created/updated in that order.

Hence a single release is created with all the objects for the chart and its dependencies.

#### Templates and Values

Helm Chart templates are written in the [Go template language](https://golang.org/pkg/text/template/), with the addition
of 50 or so add-on template functions from the [Sprig library](https://github.com/Masterminds/sprig) and a few other
[specialized functions](https://helm.sh/docs/howto/charts_tips_and_tricks/).

All template files are stored in a chart's `templates/` folder. When Helm renders the charts, it will pass every file in
that directory through the template engine.

**Values for the templates are supplied two ways**:

1. [**Compile-Time**] Chart developers may supply a file called `values.yaml` inside of a chart. This file can contain
   default values.
2. [**Run-Time**] Chart users may supply a YAML file that contains values. This can be provided on the command line with
   `helm install`.

When a user supplies custom values, these values will override the values in the chart's `values.yaml` file.

##### Template Files

Template files follow the standard conventions for writing Go templates (see
[the text/template Go package documentation](https://golang.org/pkg/text/template/) for details). An example template
file might look something like this:

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
    name: deis-database
    namespace: deis
    labels:
        app.kubernetes.io/managed-by: deis
spec:
    replicas: 1
    selector:
        app.kubernetes.io/name: deis-database
    template:
        metadata:
            labels:
                app.kubernetes.io/name: deis-database
        spec:
            serviceAccount: deis-database
            containers:
                - name: deis-database
                  image: {{ .Values.imageRegistry }}/postgres:{{ .Values.dockerTag }}
                  imagePullPolicy: {{ .Values.pullPolicy }}
                  ports:
                      - containerPort: 5432
                  env:
                      - name: DATABASE_STORAGE
                        value: {{ default "minio" .Values.storage }}
```

The example above is a template for a Kubernetes replication controller. It can use the following four template values
(usually defined in a `values.yaml` file):

1. **`imageRegistry`**: The source registry for the Docker image.
2. **`dockerTag`**: The tag for the docker image.
3. **`pullPolicy`**: The Kubernetes pull policy.
4. **`storage`**: The storage backend, whose default is set to "minio"

To see many working charts, check out the CNCF [Artifact Hub](https://artifacthub.io/packages/search?kind=0).

##### Predefined Values

Values that are supplied via a `values.yaml` file (or via the `--set` flag) are accessible from the `.Values` object in
a template. But there are other pre-defined pieces of data you can access in your templates.

The following values are pre-defined and are available to every template; they cannot be overridden. As with all values,
the names are _case sensitive_.

* **`Release.Name`**: The name of the release (not the chart)
* **`Release.Namespace`**: The namespace the chart was released to.
* **`Release.Service`**: The service that conducted the release.
* **`Release.IsUpgrade`**: This is set to true if the current operation is an upgrade or rollback.
* **`Release.IsInstall`**: This is set to true if the current operation is an install.
* **`Chart`**: The contents of the `Chart.yaml`. Thus, the chart version is obtainable as `Chart.Version` and the
  maintainers are in `Chart.Maintainers`
* **`Files`**: A map-like object containing all non-special files in the chart. This will not give you access to
  templates, but will give you access to additional files that are present (unless they are excluded using
  `.helmignore`). Files can be accessed using `{{ index .Files "file.name" }}` or using the `{{.Files.Get name }}`
  function. You can also access the contents of the file as `[]byte` using `{{ .Files.GetBytes }}`
* **`Capabilities`**: A map-like object that contains information about the versions of Kubernetes
  (`{{ .Capabilities.KubeVersion }}`) and the supported Kubernetes API versions
  (`{{ .Capabilities.APIVersions.Has "batch/v1" }}`)
  
> âš ï¸ Any unknown `Chart.yaml` fields will be dropped. They will not be accessible inside of the Chart object. Thus,
> `Chart.yaml` cannot be used to pass arbitrarily structured data into the template. The values file can be used for
> that, though.

##### Values Files

Considering the template in the previous section, a `values.yaml` file that supplies the necessary values would look
like this:

```yaml
imageRegistry: "quay.io/deis"
dockerTag: "latest"
pullPolicy: "Always"
storage: "s3"
```

A values file is formatted in YAML. A chart may include a default `values.yaml` file. The Helm install command allows a
user to override values by supplying additional YAML values:

```bash
$ helm install --generate-name --values=myvals.yaml wordpress
```

When values are passed in this way, they will be merged into the default values file. For example, consider a
`myvals.yaml` file that looks like this:

```yaml
storage: "gcs"
```

When this is merged with the `values.yaml` in the chart, the resulting generated content will be:

```yaml
imageRegistry: "quay.io/deis"
dockerTag: "latest"
pullPolicy: "Always"
storage: "gcs"
```

##### Scope, Dependencies, and Values

Values files can declare values for the top-level chart, as well as for any of the charts that are included in that
chart's `charts/` directory. Or, to put it differently, **a values file can supply values to the chart as well as to any
of its dependencies**. For example, the demonstration WordPress chart above has both `mysql` and `apache` as dependencies.
The values file could supply values to all of these components:

```yaml
title: "My WordPress Site" # Sent to the WordPress template

mysql:
    max_connections: 100 # Sent to MySQL
    password: "secret"

apache:
    port: 8080 # Passed to Apache
```

Charts at a higher level have access to all of the variables defined beneath. So the WordPress chart can access the
MySQL password as `.Values.mysql.password`. But lower level charts cannot access things in parent charts, so MySQL will
not be able to access the `title` property. Nor, for that matter, can it access `apache.port`.

Values are namespaced, but namespaces are pruned. So for the WordPress chart, it can access the MySQL password field as
`.Values.mysql.password`. But for the MySQL chart, the scope of the values has been reduced and the namespace prefix
removed, so it will see the password field simply as `.Values.password`.

##### Global Values

As of 2.0.0-Alpha.2, Helm supports special "global" value. Consider this modified version of the previous example:

```yaml
title: "My WordPress Site" # Sent to the WordPress template

global:
    app: MyWordPress

mysql:
    max_connections: 100 # Sent to MySQL
    password: "secret"

apache:
    port: 8080 # Passed to Apache
```

The above adds a `global` section with the value `app: MyWordPress`. This value is available to all charts as
`.Values.global.app`.

For example, the mysql templates may access app as `{{ .Values.global.app}}`, and so can the apache chart. Effectively,
the values file above is regenerated like this:

```yaml
title: "My WordPress Site" # Sent to the WordPress template

global:
    app: MyWordPress

mysql:
    global:
        app: MyWordPress
    max_connections: 100 # Sent to MySQL
    password: "secret"

apache:
    global:
        app: MyWordPress
    port: 8080 # Passed to Apache
```

This provides a way of sharing one top-level variable with all subcharts, which is useful for things like setting
`metadata` properties like labels.

If a subchart declares a global variable, that global will be passed _downward_ (to the subchart's subcharts), `not
upward_ to the parent chart. There is no way for a subchart to influence the values of the parent chart.

Also, global variables of parent charts take precedence over the global variables from subcharts.

##### Schema Files

Sometimes, a chart maintainer might want to define a structure on their values. This can be done by defining a schema in
the `values.schema.json` file. A schema is represented as a [JSON Schema](https://json-schema.org/). It might look
something like this:

```json
{
    "$schema": "https://json-schema.org/draft-07/schema#",
    "properties": {
        "image": {
            "description": "Container Image",
            "properties": {
                "repo": {
                    "type": "string"
                },
                "tag": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "name": {
            "description": "Service name",
            "type": "string"
        },
        "port": {
            "description": "Port",
            "minimum": 0,
            "type": "integer"
        },
        "protocol": {
            "type": "string"
        }
    },
    "required": [
        "protocol",
        "port"
    ],
    "title": "Values",
    "type": "object"
}
```

This schema will be applied to the values to validate it. Validation occurs when any of the following commands are
invoked:

* `helm install`
* `helm upgrade`
* `helm lint`
* `helm template`

An example of a `values.yaml` file that meets the requirements of this schema might look something like this:

```yaml
name: frontend
protocol: https
port: 443
```

**Note that the schema is applied to the final `.Values` object**, and not just to the `values.yaml` file. This means
that the following yaml file is valid, given that the chart is installed with the appropriate `--set` option shown
below.

```yaml
name: frontend
protocol: https
```

```bash
helm install --set port=443
```

Furthermore, the final `.Values` object is checked against all subchart schemas. This means that restrictions on a
subchart can't be circumvented by a parent chart. This also works backwards - if a subchart has a requirement that is
not met in the subchart's `values.yaml` file, the parent chart must satisfy those restrictions in order to be valid.

#### Custom Resource Definitions (CRDs)

Kubernetes provides a mechanism for declaring new types of Kubernetes objects. Using CustomResourceDefinitions (CRDs),
Kubernetes developers can declare custom resource types.

In Helm 3, CRDs are treated as a special kind of object. They are installed before the rest of the chart, and are
subject to some limitations.

CRD YAML files should be placed in the `crds/` directory inside of a chart. Multiple CRDs (separated by YAML start and
end markers) may be placed in the same file. Helm will attempt to load all of the files in the CRD directory into
Kubernetes.

CRD files cannot be templated. They must be plain YAML documents.

When Helm installs a new chart, it will upload the CRDs, pause until the CRDs are made available by the API server, and
then start the template engine, render the rest of the chart, and upload it to Kubernetes. Because of this ordering, CRD
information is available in the `.Capabilities` object in Helm templates, and Helm templates may create new instances of
objects that were declared in CRDs.

For example, if your chart had a CRD for CronTab in the `crds/` directory, you may create instances of the CronTab kind
in the `templates/` directory:

```
crontabs/
  Chart.yaml
  crds/
    crontab.yaml
  templates/
    mycrontab.yaml
```

The `crontab.yaml` file must contain the CRD with no template directives:

```yaml
kind: CustomResourceDefinition
metadata:
    name: crontabs.stable.example.com
spec:
    group: stable.example.com
    versions:
        - name: v1
          served: true
          storage: true
    scope: Namespaced
    names:
        plural: crontabs
        singular: crontab
        kind: CronTab

```

Then the template `mycrontab.yaml` may create a new `CronTab` (using templates as usual):

```yaml
apiVersion: stable.example.com
kind: CronTab
metadata:
    name: {{ .Values.name }}
spec:
    # ...
```

Helm will make sure that the `CronTab` kind has been installed and is available from the Kubernetes API server before it
proceeds installing the things in `templates/`.

##### Limitations on CRDs

Unlike most objects in Kubernetes, CRDs are installed globally. For that reason, Helm takes a very cautious approach in
managing CRDs. CRDs are subject to the following limitations:

* CRDs are never reinstalled. If Helm determines that the CRDs in the `crds/` directory are already present (regardless
  of version), Helm will not attempt to install or upgrade.
* CRDs are never installed on upgrade or rollback. **Helm will only create CRDs on installation operations**.
* CRDs are never deleted. Deleting a CRD automatically deletes all of the CRD's contents across all namespaces in the
  cluster. Consequently, Helm will not delete CRDs.

Operators who want to upgrade or delete CRDs are encouraged to do this manually and with great care.

#### Using Helm to Manage Charts

The `helm` tool has several commands for working with charts.

It can create a new chart for you:

```bash
$ helm create mychart
Created mychart/
```

Once you have edited a chart, helm can package it into a chart archive for you:

```bash
$ helm package mychart
Archived mychart-0.1.-.tgz
```

You can also use helm to help you find issues with your chart's formatting or information:

```bash
$ helm lint mychart
No issues found
```

#### Chart Repositories

A _chart repository_ is an HTTP server that houses one or more packaged charts. While `helm` can be used to manage local
chart directories, when it comes to sharing charts, the preferred mechanism is a chart repository.

Any HTTP server that can serve YAML files and tar files and can answer GET requests can be used as a repository server.
The Helm team has tested some servers, including Google Cloud Storage with website mode enabled, and S3 with website
mode enabled.

A repository is characterized primarily by the presence of a special file called **index.yaml** that has a list of all
of the packages supplied by the repository, together with metadata that allows retrieving and verifying those packages.

On the client side, repositories are managed with the `helm repo` commands. However, Helm does not provide tools for
uploading charts to remote repository servers. This is because doing so would add substantial requirements to an
implementing server, and thus raise the barrier for setting up a repository.

#### Chart Starter Packs

The `helm create` command takes an optional `--starter` option that lets you specify a "starter chart".

Starters are just regular charts, but are located in `$XDG_DATA_HOME/helm/starters`. As a chart developer, you may
author charts that are specifically designed to be used as starters. Such charts should be designed with the following
considerations in mind:

* The `Chart.yaml` will be overwritten by the generator.
* Users will expect to modify such a chart's contents, so documentation should indicate how users can do so.
* All occurrences of `<CHARTNAME>` will be replaced with the specified chart name so that starter charts can be used as
  templates.

### Accessing Charts on Rancher

From the top-left menu select "Apps & Marketplace" and you will be taken to the Charts page.

The charts page contains all Rancher, Partner, and Custom Charts

* Rancher tools such as Logging or Monitoring are included under the Rancher label
* Partner charts reside under the Partners label
* Custom charts will show up under the name of the repository

All three types are deployed and managed in the same way.

### Chart Template for Rancher

#### Recap: Charts

As described in the [Charts](#charts), Helm charts are structured like this:

```
helm-charts/
  Chart.yaml
  values.yaml
  charts/
  templates/
  ...
```

The `templates/` directory is for template files. **When Helm evaluates a chart, it will send all of the files in the
`templates/` directory through the template rendering engine. It then collects the results of those templates and sends
them on to Kubernetes**.

The `values.yaml` file is also important to templates. This file contains the default values for a chart. These values
may be overridden by users during `helm install` or `helm upgrade`.

The `Chart.yaml` file contains a description of the chart. You can access it from within a template. The `charts/`
directory may contain other charts (which we call "subcharts"). We will see how those work when it comes to template
rendering.

#### Creating Chart

Let's create a simple chart called "helm-charts", and then we'll create some templates inside of the chart.

```bash
$ helm create helm-charts
Creating helm-charts
```

##### A Quick Glimpse of helm-charts/templates/

If you take a look at the `helm-charts/templates/` directory, you'll notice a few files already there.

* `NOTES.txt`: The "help text" for your chart. This will be displayed to your users when they run `helm install`.
* **`deployment.yaml`**: **A basic manifest for creating a Kubernetes deployment**
* **`service.yaml`**: **A basic manifest for creating a service endpoint for your deployment**
* `_helpers.tpl`: A place to put template helpers that you can re-use throughout the chart

And what we're going to do is... remove them all! That way we can work through our tutorial from scratch. We'll actually
create our own `NOTES.txt` and `_helpers.tpl` as we go.

```bash
$ rm -rf helm-charts/templates/*
```

When you're writing production grade charts, having basic versions of these charts can be really useful. So in your
day-to-day chart authoring, you probably won't want to remove them.

#### A First Template

The first template we are going to create will be a "ConfigMap". **In Kubernetes, a ConfigMap is simply an object for
storing configuration data**. Other things, like pods, can access the data in a ConfigMap

Let's begin by creating a file called `helm-charts/templates/configmap.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
    name: helm-charts-configmap
data:
    myvalue: "Hello World"
```

[Installing and Deploying helm-charts](#old-versionbefore-25) gives the following result:

![configmap-test-2.png not loaded property]({{ "/assets/img/configmap-test-2.png" | relative_url}})

#### ConfigMaps (A Kubernetes Resource)

A ConfigMap is an API object used to store non-confidential data in key-value pairs.
[Pods](https://kubernetes.io/docs/concepts/workloads/pods/) can **consume ConfigMaps as environment variables**,
command-line arguments, or as **configuration files in a
[volume](https://kubernetes.io/docs/concepts/storage/volumes/)**.

A ConfigMap allows you to decouple environment-specific configuration from your
[container images](https://kubernetes.io/docs/reference/glossary/?all=true#term-image), so that your applications are
easily portable.

> âš ï¸ ConfigMap does not provide secrecy or encryption. If the data you want to store are confidential, use a
> [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) rather than a ConfigMap, or use additional (third
> party) tools to keep your data private.

##### Motivation

**Use a ConfigMap for setting configuration those data that are separate from application code**.

For example, imagine that you are developing an application that you can run on your own computer (for development) and
in the cloud (to handle real traffic). You write the code to look in an environment variable named `DATABASE_HOST`.
Locally, you set that variable to `localhost`. In the cloud, you set it to refer to a Kubernetes
[Service](#service-a-kubernetes-resource) that exposes the database component to your cluster. This lets you fetch a
container image running in the cloud and debug the exact same code locally if needed.

A ConfigMap is not designed to hold large chunks of data. The data stored in a ConfigMap cannot exceed 1 MiB. If you
need to store settings that are larger than this limit, you may want to consider mounting a volume or use a separate
database or file service.

##### ConfigMap Object

A ConfigMap is an API [object](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/)
that lets you store configuration for other objects to use. Unlike most Kubernetes objects that have a `spec`, a
ConfigMap has `data` and `binaryData` fields. These fields accept key-value pairs as their values. Both the `data` field
and the `binaryData` are optional. The `data` field is designed to contain UTF-8 byte sequences while the `binaryData`
field is designed to contain binary data as base64-encoded strings.

The name of a ConfigMap must be a valid
[DNS subdomain name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

Each key under the `data` or the `binaryData` field must consist of alphanumeric characters, `-`, `_` or `.`. The keys
stored in `data` must not overlap with the keys in the `binaryData` field.

Starting from v1.19, you can add an `immutable` field to a ConfigMap definition to create an
[immutable ConfigMap](#immutable-configmaps).

##### ConfigMaps and Pods

You can write a Pod spec that refers to a ConfigMap and configures the container(s) in that Pod based on the data in the
ConfigMap. The Pod and the ConfigMap must be in the same
[namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces).

> âš ï¸ The `spec` of a [static Pod](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/) cannot refer to
> a ConfigMap or any other API objects.

Here's an example ConfigMap that has some keys with single values, and other keys where the value looks like a fragment
of a configuration format.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: game-demo
data:
  # property-like keys; each key maps to a simple value
  player_initial_lives: "3"
  ui_properties_file_name: "user-interface.properties"

  # file-like keys
  game.properties: |
    enemy.types=aliens,monsters
    player.maximum-lives=5    
  user-interface.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true    
```

There are four different ways that you can use a ConfigMap to configure a container inside a Pod:

1. Inside a container command and args
2. Environment variables for a container
3. Add a file in read-only volume, for the application to read
4. Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap

These different methods lend themselves to different ways of modeling the data being consumed. For the first three
methods, the [kubelet](https://kubernetes.io/docs/reference/generated/kubelet) uses the data from the ConfigMap when it
launches container(s) for a Pod.

The fourth method means you have to write code to read the ConfigMap and its data. However, because you're using the
Kubernetes API directly, your application can subscribe to get updates whenever the ConfigMap changes, and react when
that happens. By accessing the Kubernetes API directly, this technique also lets you access a ConfigMap in a different
namespace.

Here's an example Pod that uses values from game-demo to configure a Pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      command: ["sleep", "3600"]
      env:
        # Define the environment variable
        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here
                                     # from the key name in the ConfigMap.
          valueFrom:
            configMapKeyRef:
              name: game-demo           # The ConfigMap this value comes from.
              key: player_initial_lives # The key to fetch.
        - name: UI_PROPERTIES_FILE_NAME
          valueFrom:
            configMapKeyRef:
              name: game-demo
              key: ui_properties_file_name
      volumeMounts:
      - name: config
        mountPath: "/config"
        readOnly: true
  volumes:
    # You set volumes at the Pod level, then mount them into containers inside that Pod
    - name: config
      configMap:
        # Provide the name of the ConfigMap you want to mount.
        name: game-demo
        # An array of keys from the ConfigMap to create as files
        items:
        - key: "game.properties"
          path: "game.properties"
        - key: "user-interface.properties"
          path: "user-interface.properties"
```

A ConfigMap doesn't differentiate between single line property values and multi-line file-like values. What matters is
how Pods and other objects consume those values.

For this example, defining a volume and mounting it inside the `demo` container as `/config` creates two files,
`/config/game.properties` and `/config/user-interface.properties`, even though there are four keys in the ConfigMap.
This is because the Pod definition specifies an `items` array in the `volumes` section. If you omit the `items` array
entirely, every key in the ConfigMap becomes a file with the same name as the key, and you get 4 files.

##### Using ConfigMaps

ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other parts of the system, without being
directly exposed to the Pod. For example, ConfigMaps can hold data that other parts of the system should use for
configuration.

The most common way to use ConfigMaps is to configure settings for containers running in a Pod in the same namespace.
You can also use a ConfigMap separately.

For example, you might encounter [addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/) or
[operators](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) that adjust their behavior based on a
ConfigMap.

###### Using ConfigMaps as Files from a Pod

To consume a ConfigMap in a volume in a Pod:

1. Create a ConfigMap or use an existing one. Multiple Pods can reference the same ConfigMap.
2. Modify your Pod definition to add a volume under `.spec.volumes[]`. Name the volume anything, and have a
   `.spec.volumes[].configMap.name` field set to reference your ConfigMap object.
3. Add a `.spec.containers[].volumeMounts[]` to each container that needs the ConfigMap. Specify
   `.spec.containers[].volumeMounts[].readOnly = true` and `.spec.containers[].volumeMounts[].mountPath` to an unused
   directory name where you would like the ConfigMap to appear.
4. Modify your image or command line so that the program looks for files in that directory. Each key in the ConfigMap
   `data` map becomes the filename under `mountPath`.

This is an example of a Pod that mounts a ConfigMap in a volume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap
```

Each ConfigMap you want to use needs to be referred to in `.spec.volumes`.

If there are multiple containers in the Pod, then each container needs its own `volumeMounts` block, but only one
`.spec.volumes` is needed per ConfigMap.

###### Using ConfigMap for Initializing MySQL

According to the MySQL Docker image [README](https://hub.docker.com/_/mysql/), the part that is relevant to data
initialization on container start-up is to ensure all your initialization files are mount to the container's
`/docker-entrypoint-initdb.d` folder.

We can define your initial data in a ConfigMap, and mount the corresponding volume in our pod/deployment like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
  - name: mysql
    image: mysql        
    ports:
      - containerPort: 3306
    volumeMounts:
      - name: mysql-initdb
        mountPath: /docker-entrypoint-initdb.d
  volumes:
    - name: mysql-initdb
      configMap:
        name: mysql-initdb-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-initdb-config
data:
  initdb.sql: |
    CREATE TABLE friends (id INT, name VARCHAR(256), age INT, gender VARCHAR(3));
    INSERT INTO friends VALUES (1, 'John Smith', 32, 'm');
    INSERT INTO friends VALUES (2, 'Lilian Worksmith', 29, 'f');
    INSERT INTO friends VALUES (3, 'Michael Rupert', 27, 'm');
```

###### Mounted ConfigMaps are Updated Automatically

When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well. The kubelet
checks whether the mounted ConfigMap is fresh on every periodic sync. However, the kubelet uses its local cache for
getting the current value of the ConfigMap. The type of the cache is configurable using the
`ConfigMapAndSecretChangeDetectionStrategy` field in the
[KubeletConfiguration struct](https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/). A ConfigMap can
be either propagated by watch (default), ttl-based, or by redirecting all requests directly to the API server. As a
result, the total delay from the moment when the ConfigMap is updated to the moment when new keys are projected to the
Pod can be as long as the kubelet sync period + cache propagation delay, where the cache propagation delay depends on
the chosen cache type (it equals to watch propagation delay, ttl of cache, or zero correspondingly).

> ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.

###### Immutable ConfigMaps

The Kubernetes feature _Immutable Secrets_ and _ConfigMaps_ provides an option to set individual Secrets and ConfigMaps
as immutable. For clusters that extensively use ConfigMaps (at least tens of thousands of unique ConfigMap to Pod
mounts), preventing changes to their data has the following advantages:

* protects you from accidental (or unwanted) updates that could cause applications outages
* improves performance of your cluster by significantly reducing load on kube-apiserver, by closing watches for 
  ConfigMaps marked as immutable.

This feature is controlled by the `ImmutableEphemeralVolumes`
[feature gate](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/). You can create an
immutable ConfigMap by setting the `immutable` field to `true`. For example:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  ...
data:
  ...
immutable: true
```

Once a ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the
`data` or the `binaryData` field. You can only delete and recreate the ConfigMap. Because existing Pods maintain a mount
point to the deleted ConfigMap, it is recommended to recreate these pods.

#### Deployment (A Kubernetes Resource)

**A "deployment" is a kubernetes concept** and is, therefore, not discussed in Helm Charts.

A _Deployment_ provides declarative updates for [Pods](https://kubernetes.io/docs/concepts/workloads/pods/) and
[ReplicaSets](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/).

##### Creating a Deployment

The following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
    name: nginx-deployment
    labels:
        app: nginx
spec:
    replicas: 3
    selector:
        matchLabels:
            app: nginx
    template:
        metadata:
            labels:
                app: nginx
        spec:
            containers:
             - name: nginx
               image: nginx:1.14.2
               ports:
               - containerPort: 80
```

In this example:

* A Deployment named `nginx-deployment` is created, indicated by the `.metadata.name` field.
* The Deployment creates three replicated Pods, indicated by the `.spec.replicas` field.
* The `.spec.selector` field defines how the Deployment finds which Pods to manage. In this case, you select a label
  that is defined in the Pod template (`app: nginx`). However, more sophisticated selection rules are possible, as long
  as the Pod template itself satisfies the rule.
  > The `.spec.selector.matchLabels` field is a map of {key,value} pairs. A single {key,value} in the `matchLabels` map
  > is equivalent to an element of `matchExpressions`, whose key field is "key", the operator is "In", and the values
  > array contains only "value". All of the requirements, from both `matchLabels` and `matchExpressions`, must be
  > satisfied in order to match.
* The `template` field contains the following sub-fields:
  - The Pods are labeled `app: nginx`using the `.metadata.labels` field.
  - The Pod template's specification, or `.template.spec` field, indicates that the Pods run one container, nginx, which
    runs the nginx Docker Hub image at version 1.14.2.
  - Create one container and name it nginx using the `.spec.template.spec.containers[0].name` field.
    
##### Define Environment Variables for a Container

When you create a Pod, you can set environment variables for the containers that run in the Pod. Those variables shows
up in Rancher in the "Environment Variables" section in the workload page of a container

![rancher-deployment-env-variables.png not loaded property]({{ "/assets/img/rancher-deployment-env-variables.png" | relative_url}})

To set environment  variables, include the `env` or `envFrom` field in the configuration file:

In this the example below, you create a Pod that runs one container. The configuration file for the Pod defines an
environment variable with name "DEMO_GREETING" and value "Hello from the environment". Here is the configuration
manifest for the Pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: envar-demo
    labels:
        purpose: demonstrate-envars
spec:
    containers:
    - name: envar-demo-container
      image: gcr.io/google-samples/node-hello:1.0
      env:
      - name: DEMO_GREETING
        value: "Hello from the environment"
      - name: DEMO_FAREWELL
        value: "Such a sweet sorrow"
```

##### Using Environment Variables Inside of Your Config

Environment variables that you define in a Pod's configuration can be used elsewhere in the configuration, for example
in commands and arguments that you set for the Pod's containers. In the example configuration below, the `GREETING`,
`HONORIFIC`, and `NAME` environment variables are set to "Warm greetings to", "The Most Honorable", and "Kubernetes",
respectively. Those environment variables are then used in the CLI arguments passed to the env-print-demo container.

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: print-greeting
spec:
    containers:
    - name: env-print-demo
      image: bash
      env:
      - name: GREETING
        value: "Warm greetings to"
      - name: HONORIFIC
        value: "The Most Honorable"
      - name: NAME
        value: "Kubernetes"
      command: ["echo"]
      args: ["$(GREETING) $(HONORIFIC) $(NAME)"]
```

Upon creation, the command `echo Warm greetings to The Most Honorable Kubernetes` is run on the container.

#### Service (A Kubernetes Resource)

**A "service" is a kubernetes concept** and is, therefore, not discussed in Helm Charts.

Service is an abstract way to expose an application running on a set of Pods as a network service.  With Kubernetes you
don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own
IP addresses and a single DNS name for a set of Pods, and can load-balance across them.

##### Motivation

Kubernetes [Pods](https://kubernetes.io/docs/concepts/workloads/pods/) are created and destroyed to match the state of
your cluster. Pods are nonpermanent resources. If you use a [Deployment](#deployment-a-kubernetes-resource) to run your
app, it can create and destroy Pods dynamically.

Each Pod gets its own IP address however in a Deployment, the set of Pods running in one moment in time could be
different from the set of Pods running that application a moment later.

This leads to a problem: if some set of Pods (call them "backends") provides functionality to other Pods (call them
"frontends") inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so
that the frontend can use the backend part of the workload?

The answer is _services_

In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them
(sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a
**selector**. To learn about other ways to define Service endpoints, see 
[Services without selectors](#services-without-selectors).

For example, consider a stateless image-processing backend which is running with 3 replicas. Those replicas are
fungible - frontends do not care which backend they use. While the actual Pods that compose the backend set may change,
the frontend clients should not need to be aware of that, nor should they need to keep track of the set of backends
themselves.

The Service abstraction enables this decoupling.

###### Cloud-native Service Discovery

If you're able to use Kubernetes APIs for service discovery in your application, you can query the
[API server](https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver) for Endpoints, that get updated
whenever the set of Pods in a Service changes.

For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application
and the backend Pods.

##### Defining a Service

A Service in Kubernetes is a REST object, similar to a Pod. Like all of the REST objects, you can `POST` a Service
definition to the API server to create a new instance. The name of a Service object must be a valid
[RFC 1035 label name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names).

For example, suppose you have a set of Pods where each listens on TCP port 9376 and contains a label `app=MyApp`:

```yaml
apiVersion: v1
kind: Service
metadata:
    name: my-service
spec:
    selector:
        app: MyApp
    ports:
        - protocol: TCP
          port: 80
          targetPort: 9376
```

The specification above creates a new Service object named "my-service", which targets TCP port 9376 on any Pod with the
`app=MyApp` label.

Kubernetes assigns this Service an IP address (sometimes called the "cluster IP"), which is used by the Service proxies
(see [Virtual IPs and service proxies](#virtual-ips-and-service-proxies) below).

**The controller for the Service selector continuously scans for Pods that match its selector, and then POSTs any
updates to an Endpoint object also named "my-service"**.

> ðŸ“‹ï¸ A Service can map any incoming `port` to a `targetPort`. By default and for convenience, the `targetPort` is set to
> the same value as the `port` field.

Port definitions in Pods have names, and you can reference these names in the `targetPort` attribute of a Service. This
works even if there is a mixture of Pods in the Service using a single configured name, with the same network protocol
available via different port numbers. This offers a lot of flexibility for deploying and evolving your Services. For
example, you can change the port numbers that Pods expose in the next version of your backend software, without breaking
clients.

The default protocol for Services is TCP; you can also use any other
[supported protocol](https://kubernetes.io/docs/concepts/services-networking/service/#protocol-support).

As many Services need to expose more than one port, Kubernetes supports multiple port definitions on a Service object.
Each port definition can have the same protocol, or a different one.

###### Services without Selectors

```yaml
apiVersion: v1
kind: Service
metadata:
    name: my-service
spec:
    ports:
        - protocol: TCP
          port: 80
          targetPort: 9376
```

Because this Service has no selector, the corresponding Endpoints object is not created automatically. You can manually
map the Service to the network address and port where it's running, by adding an Endpoints object manually:

```yaml
apiVersion: v1
kind: Endpoints
metadata:
    name: my-service
subsets:
    - addresses:
        - ip: 192.0.2.42
      ports:
        - port: 9376
```

The name of the Endpoints object must be a valid
[DNS subdomain name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

> âš ï¸ The endpoint IPs must not be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local (169.254.0.0/16 and
> 224.0.0.0/24 for IPv4, fe80::/64 for IPv6). 
>
> Endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services, because
> [kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) doesn't support virtual
> IPs as a destination.

Accessing a Service without a selector works the same as if it had a selector. In the example above, traffic is routed
to the single endpoint defined in the YAML: `192.0.2.42:9376` (TCP).

An _ExternalName_ Service is a special case of Service that does not have selectors and uses DNS names instead. For more
information, see the [ExternalName](https://kubernetes.io/docs/concepts/services-networking/service/#externalname)
section later in this document.

##### Virtual IPs and Service Proxies

#### Exposing Service Publicly

By having a service "publicly" exposed, we mean having a rancher app running on Kubernetes, and our Mac/Windows/Linux
could connect to the app remotely. To achieve that, we will use [deployment](#deployment-a-kubernetes-resource) and
[service](#service-a-kubernetes-resource).

Our service YAML file would look like this:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: service-foo
spec:
  type: NodePort
  selector:
    app: service-foo
  ports:
    - name: web-interface-port
      port: 8090
      targetPort: 8090
      protocol: TCP
status:
  loadBalancer: {}
```

The deployment YAML would be written as follows

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-foo
  labels:
    app: service-foo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-foo
  template:
    metadata:
      labels:
        app: service-foo
    spec:
      containers:
        - name: service-foo
          image: "..."
          imagePullPolicy: {{ .Values.global.image.pullPolicy }}
          ports:
          - name: web-interface-port
            containerPort: 8090
            protocol: TCP

```

In the deployment config, we will have a service listening on port 8090, which we will expose externally. The service
config takes that port (`targetPort`) and exposed it as `port`.

> âš ï¸ It is very important to have the selector (`service-foo`) match in both deployment and serice file, otherwise the
> port 8090 won't have a corresponding public port (image!!!)

Explain... & https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/

### Repositories

#### Old Version(Before 2.5)

Repositories have global scop and can be accessed in the following way: 

![rancher-create-catelog.png not loaded property]({{ "/assets/img/rancher-create-catelog.png" | relative_url}})

Creating repository:

![rancher-create-catelog2.png not loaded property]({{ "/assets/img/rancher-create-catelog2.png" | relative_url}})

> âš ï¸ It is important to check "Use private catalog" and provide the config with a git repo username & password,
> otherwise the following error will show up
> 
> ![rancher-catlog-error.png not loaded property]({{ "/assets/img/rancher-catlog-error.png" | relative_url}})

A successfully created repository should show up as something like thits below:

![rancher-catlog-error-fixed.png not loaded property]({{ "/assets/img/rancher-catlog-error-fixed.png" | relative_url}})

> âš ï¸ It is crucial to **manually** refresh the catalog in order to keep rancher in sync with the git repo
> 
> ![rancher-manual-refresh.png not loaded property]({{ "/assets/img/rancher-manual-refresh.png" | relative_url}})

#### New Version(Starting 2.5)

From the left sidebar select "Chart Repositories".

![rancher-chart-repositories.png not loaded property]({{ "/assets/img/rancher-chart-repositories.png" | relative_url}})

These items represent helm repositories, and can be either traditional helm endpoints which have an `index.yaml`, or git
repositories which will be cloned and can point to a specific branch. In order to use custom charts, simply add your
repository here and they will become available in the Charts tab under the name of the repository.

### Deployment and Upgrades

From the "Charts" tab select a Chart to install.

![rancher-chart-deployment.png not loaded property]({{ "/assets/img/rancher-chart-deployment.png" | relative_url}})

To view all recent changes, go to the "Recent Operations" tab. From there you can view the call that was made,
conditions, events, and logs.

After installing a chart, you can find it in the "Installed Apps" tab.

Most Rancher tools have additional pages located in the toolbar below the "Apps & Marketplace" section to help manage
and use the features. These pages include links to dashboards, forms to easily add Custom Resources, and additional
information.

## Troubleshooting

### Error: Kubernetes cluster unreachable with helm 3.0

Try setting the KUBECONFIG environment variable.

```bash
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
```

