---
layout: post
title: What is WebGL
tags: [WebGL]
category: FINALIZED
color: rgb(204, 50, 50)
feature-img: "assets/img/post-cover/1-cover.png"
thumbnail: "assets/img/post-cover/1-cover.png"
author: QubitPi
excerpt_separator: <!--more-->
---

<!--more-->

In early years, Java applications - as a combination of applets and JOGL - were used to process 3D graphics on the Web
by addressing the GPU (Graphical Processing Unit). As applets require a JVM to run, it became difficult to rely on Java 
applets. A few years later, people stopped using Java applets.

The Stage3D APIs provided by Adobe (Flash, AIR) offered GPU hardware accelerated architecture. Using these technologies, 
programmers could develop applications with 2D and 3D capabilities on web browsers as well as on IOS and Android 
platforms. Since Flash was a proprietary software, it was not used as web standard.

In March 2011, WebGL was released. It is an openware that can run without a JVM. It is completely controlled by the web 
browser.

The new release of HTML 5 has several features to support 3D graphics such as 2D Canvas, WebGL, SVG, 3D CSS transforms, 
and SMIL. In this tutorial, we will be covering the basics of WebGL.

* TOC
{:toc}


What is OpenGL?
---------------

[OpenGL (Open Graphics Library)](https://en.wikipedia.org/wiki/OpenGL) is a cross-language, cross-platform API for 2D
and 3D graphics. It is a collection of commands. The table below lists a set of technologies related to OpenGL.

| **API**   | **Technology Used**                                                                                                                                                                                                                                             |
|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| OpenGL ES | It is the library for 2D and 3D graphics on embedded systems - including consoles, phones, appliances, and vehicles. It is maintained by the Khronos Group www.khronos.org                                                                                      |
| JOGL      | It is the Java binding for OpenGL. It is maintained by jogamp.org.                                                                                                                                                                                              |
| WebGL     | It is the JavaScript binding for OpenGL. It is maintained by the [Khronos Group](https://www.khronos.org/).                                                                                                                                                     |
| OpenGLSL  | OpenGL Shading Language. It is a programming language which is a companion to OpenGL 2.0 and higher. It is a part of the core OpenGL 4.4 specification and an API specifically tailored to embedded systems such as those present on mobile phones and tablets. |

> ðŸ“‹ In WebGL, we use GLSL to write shaders


What is WebGL?
--------------

WebGL (Web Graphics Library) is the new standard for 3D graphics on the Web, It is designed for the purpose of rendering
2D graphics and interactive 3D graphics. It is derived from OpenGL's ES 2.0 library which is a low-level 3D API for
phones and other mobile devices. WebGL provides similar functionality of ES 2.0 (Embedded Systems) and performs well on 
modern 3D graphics hardware.

As a JavaScript API, WebGL can be used with HTML5. The code is written within the `<canvas>` tag of HTML5. It is also a 
specification that allows Internet browsers access to Graphic Processing Units (GPUs) on those computers where they were 
used.

### Rendering

Rendering is the process of generating an image from a model using computer programs. In graphics, a virtual scene is 
described using information like geometry, viewpoint, texture, lighting, and shading, which is passed through a render 
program. The output of this render program will be a digital image.

There are two types of rendering

1. **Software Rendering** All the rendering calculations are done with the help of CPU.
2. **Hardware Rendering** All the graphics computations are done by the GPU (Graphical Processing Unit).

Rendering can be done locally or remotely. If the image to be rendered is way too complex, then rendering is done
remotely on a dedicated server having enough of hardware resources required to render complex scenes. It is also called 
**server-based rendering**. Rendering can also be done locally by the CPU, i.e. **client-based rendering**.

WebGL follows a client-based rendering approach to render 3D scenes. All the processing required to obtain an image is 
performed locally **using the client's graphics hardware**.

### GPU

According to NVIDIA, a GPU is "a single chip processor with integrated transform, lighting, triangle setup/clipping, and 
rendering engines capable of processing a minimum of 10 million polygons per second." Unlike multi-core processors with
a few cores optimized for sequential processing, a GPU consists of thousands of smaller cores that process parallel 
workloads efficiently. Therefore, the GPU accelerates the creation of images in a frame buffer (a portion of ram which 
contains a complete frame data) intended for output to a display.

![Error loading cpu-vs-gpu.png]({{ "/assets/img/cpu-vs-gpu.png" | relative_url}})

#### GPU Accelerated Computing

In GPU accelerated computing, the application is loaded into the CPU. Whenever it encounters a compute-intensive portion
of the code, then that portion of code will be loaded and run on the GPU. It gives the system the ability to process 
graphics in an efficient way.

![Error loading gpu-accelerated-computing.png]({{ "/assets/img/gpu-accelerated-computing.png" | relative_url}})

GPU will have a separate memory and it runs multiple copies of a small portion of the code at a time. The GPU processes
all the data which is in its local memory, not the central memory. Therefore, the data that is needed to be processed by 
the GPU should be loaded/copied to the GPU memory and then be processed.

In the systems having the above architecture, the communication overhead between the CPU and GPU should be reduced to 
achieve faster processing of 3D programs. For this, we have to copy all the data and keep it on the GPU, instead of 
communicating with the GPU repeatedly.

### Advantages of WebGL

* **JavaScript Integrated** WebGL applications are written in JavaScript. Using these applications, you can directly
  interact with other elements of the HTML Document. You can also use other JavaScript libraries (e.g. JQuery) and HTML 
  technologies to enrich the WebGL application.
* ** Increasing support with mobile browsers** WebGL also supports Mobile browsers such as iOS safari, Android Browser,
  and Chrome for Android.
* **Open source** We can access the source code of WebGL and understand how it works and how it was developed.
* **No need for compilation** JavaScript is a half-programming and half-HTML component. To execute this script, there is 
  no need to compile the file. Instead, you can directly open the file using any of the browsers and check the result. 
  Since WebGL applications are developed using JavaScript, there is no need to compile WebGL applications.
* **Automatic memory management** JavaScript supports automatic memory management. There is no need for manual
  allocation of memory. WebGL inherits this feature of JavaScript.
* **Easy to set up** Since WebGL is integrated within HTML 5, there is no need for additional set up. To write a WebGL 
  application, all that you need is a text editor and a web browser


Getting Started with WebGL
--------------------------

WebGL enables web content to use an API based on OpenGL ES 2.0 to perform 2D and 3D rendering in an HTML `canvas` in 
browsers that support it without the use of plug-ins.

WebGL programs consist of _control code_ written in JavaScript and _shader code_ (GLSL) that is executed on a computer's 
Graphics Processing Unit (GPU). WebGL elements can be mixed with other HTML elements and composited with other parts of 
the page or page background.

> ðŸ“‹ It's worth noting here that there are a number of frameworks available that encapsulate WebGL's capabilities,
> making it easier to build 3D applications and games, such as [THREE.js](https://threejs.org/) and
> [BABYLON.js](https://www.babylonjs.com/).

### Preparing to Render in 3D

The first thing you need in order to use WebGL for rendering is a **canvas**. The HTML fragment below declares a canvas 
that our sample will draw into.

```html
<body>
    <canvas id="glCanvas" width="640" height="480"></canvas>
</body>
```

#### Preparing the WebGL Context

The `main()` function in our JavaScript code, is called when our script is loaded. Its purpose is to set up the WebGL 
context and start rendering content.

```html
function main() {
    const canvas = document.querySelector("#glCanvas");
    // Initialize the GL context
    const gl = canvas.getContext("webgl");
    
    // Only continue if WebGL is available and working
    if (gl === null) {
        alert("Unable to initialize WebGL. Your browser or machine may not support it.");
        return;
    }
    
    // Set clear color to black, fully opaque
    gl.clearColor(0.0, 0.0, 0.0, 1.0);
    // Clear the color buffer with specified clear color
    gl.clear(gl.COLOR_BUFFER_BIT);
}

window.onload = main;
```

The first thing we do here is to obtain a reference to the canvas, assigning it to a variable named `canvas`.

Once we have the canvas, we try to get a
[WebGLRenderingContext](https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext) for it by calling 
[getContext()](https://developer.mozilla.org/en-US/docs/Web/API/HTMLCanvasElement/getContext) and passing it the string 
"webgl". If the browser does not support WebGL, getContext() will return null in which case we display a message to the 
user and exit.

If the context is successfully initialized, the variable `gl` is our reference to it. In this case, we set the clear
color to black, and clear the context to that color (redrawing the canvas with the background color).

At this point, you have enough code that the WebGL context should successfully initialize, and you should wind up with a 
big black, empty box, ready and waiting to receive content.

### Adding 2D Content to a WebGL Context

The most important thing to understand before we get started is that even though we're only rendering a 2D object in
this example, we're still drawing in 3D space. It's just we're drawing it and we're putting it directly in front of the 
camera perpendicular to the view direction. We need to define the shaders that will create the color for our simple
scene as well as draw our object. These will establish how a square plane appears on the screen.

#### The Shaders

A **shader** is a program, written using the
[OpenGL ES Shading Language (GLSL)](https://www.khronos.org/registry/OpenGL/specs/es/3.2/GLSL_ES_Specification_3.20.pdf), 
that takes information about the vertices that make up a shape and generates the data needed to render the pixels onto
the screen: namely, the positions of the pixels and their colors.

There are two shader functions run when drawing WebGL content: the vertex shader and the fragment shader. You write these 
in GLSL and pass the text of the code into WebGL to be compiled for execution on the GPU. Together, a set of vertex and 
fragment shaders is called a **shader program**.

Let's take a quick look at the two types of shader, with the example in mind of drawing a 2D shape into the WebGL
context.

##### Vertex Shader

Each time a shape is rendered, the vertex shader is run for each vertex in the shape. Its job is to transform the input 
vertex from its original coordinate system into the clip space coordinate system used by WebGL, in which each axis has a 
range from -1.0 to 1.0, regardless of aspect ratio, actual size, or any other factors.

> ðŸ“‹ A **vertex** is a point which defines the conjunction of the edges of a 3D object. It is represented by three
> floating point values each representing X, Y, Z axes respectively.

> **Clip space**
> 
> In a WebGL program, data is typically uploaded to the GPU with its own coordinate system and then the vertex shader 
> transforms those points into a special coordinate system known as clip space. Any data which extends outside of the
> clip space is clipped off and not rendered. However, if a triangle straddles the border of this space then it is
> chopped up into new triangles, and only the parts of the new triangles that are in clip space are kept
> 
> ![Error loading clip-space-graph.svg]({{ "/assets/img/clip-space-graph.svg" | relative_url}})
> 
> The graphic above is a visualization of the clip space that all of the points must fit into. It is a cube two units on 
> each side, with one corner at (-1,-1,-1) and the opposite corner at (1,1,1). The center of the cube is the point 
> (0,0,0). This 8 cubic meter coordinate system used by clip space is known as **normalized device coordinates (NDC)**.
> You may encounter that term from time to time while researching and working with WebGL code

The vertex shader must perform the needed transforms on the vertex's position, make any other adjustments or
calculations it needs to make on a per-vertex basis, then return the transformed vertex by saving it in a special
variable provided by GLSL, called **gl_Position**.

The vertex shader can, as needed, also do things like determine the coordinates within the face's texture of the
[**texel**](https://developer.mozilla.org/en-US/docs/Glossary/Texel) to apply to the vertex, apply the normals to 
determine the lighting factor to apply to the vertex, and so on. This information can then be stored in **varyings** or 
**attributes** as appropriate to be shared with the fragment shader.

> **Attributes**
> 
> Attributes are GLSL variables which are only available to the vertex shader (as variables) and the JavaScript code. 
> Attributes are typically used to store color information, texture coordinates, and any other data calculated or 
> retrieved that needs to be shared between the JavaScript code and the vertex shader.
>
> ```html
> // init colors
> const vertexColors = [
>     vec4(0.0, 0.0, 0.0, 1.0),  // black
>     vec4(1.0, 0.0, 0.0, 1.0),  // red
>     vec4(1.0, 1.0, 0.0, 1.0),  // yellow
>     vec4(0.0, 1.0, 0.0, 1.0),  // green
>     vec4(0.0, 0.0, 0.0, 1.0),  // black
>     vec4(1.0, 0.0, 0.0, 1.0),  // red
>     vec4(1.0, 1.0, 0.0, 1.0),  // yellow
>     vec4(0.0, 1.0, 0.0, 1.0),  // green
> ];
> const cBuffer = gl.createBuffer();
> ```
> 
> ```html
> // continued
> // create buffer to store colors and reference it to "vColor" which is in GLSL
> gl.bindBuffer(gl.ARRAY_BUFFER, cBuffer);
> gl.bufferData(gl.ARRAY_BUFFER, flatten(vertexColors), gl.STATIC_DRAW);
> 
> const vColor = gl.getAttribLocation(program, "vColor");
> gl.vertexAttribPointer(vColor, 4, gl.FLOAT, false, 0, 0);
> gl.enableVertexAttribArray(vColor);
> ```
> 
> ```html
> //glsl
> attribute  vec4 vColor;
> 
> void main() {
>     fColor = vColor;
> }
>```

> **Varyings**
> 
> Varyings are variables that are declared by the vertex shader and used to pass data from the vertex shader to the 
> fragment shader. This is commonly used to share a vertex's
> [normal vector](https://en.wikipedia.org/wiki/Normal_(geometry)) after it has been computed by the vertex shader.

> **Uniforms**
> 
> Uniforms are set by the JavaScript code and are available to both the vertex and fragment shaders. They're used to 
> provide values that will be the same for everything drawn in the frame, such as lighting positions and magnitudes, 
> global transformation and perspective details, and so forth.

Our vertex shader below receives vertex position values from an attribute we define called `aVertexPosition`. That 
position is then multiplied by two 4x4 matrices we provide called `uProjectionMatrix` and `uModelViewMatrix`; 
`gl_Position` is set to the result. 

```html
// Vertex shader program

const vsSource = `
    attribute vec4 aVertexPosition;
  
    uniform mat4 uModelViewMatrix;
    uniform mat4 uProjectionMatrix;
  
    void main() {
      gl_Position = uProjectionMatrix * uModelViewMatrix * aVertexPosition;
    }
`;
```

##### Fragment Shader

_The fragment shader is called once for every pixel on each shape_ to be drawn, after the shape's vertices have been 
processed by the vertex shader. Its job is to determine the color of that pixel by figuring out which texel (that is,
the pixel from within the shape's texture) to apply to the pixel, getting that texel's color, then applying the 
appropriate lighting to the color. The color is then returned to the WebGL layer by storing it in the special variable 
`gl_FragColor`. That color is then drawn to the screen in the correct position for the shape's corresponding pixel.

In this case, we're returning white every time, since we're just drawing a white square, with no lighting in use.

```html
    const fsSource = `
        void main() {
            gl_FragColor = vec4(1.0, 1.0, 1.0, 1.0);
        }
    `;
```

#### Initializing the Shaders

Now that we've defined the two shaders we need to pass them to WebGL, compile them, and link them together. The code
below creates the two shaders by calling **loadShader()**, passing the type and source for the shader. It then creates a 
program, attaches the shaders and links them together. If compiling or linking fails the code displays an alert.

```html
//
// Initialize a shader program, so WebGL knows how to draw our data
//
function initShaderProgram(gl, vsSource, fsSource) {
    const vertexShader = loadShader(gl, gl.VERTEX_SHADER, vsSource);
    const fragmentShader = loadShader(gl, gl.FRAGMENT_SHADER, fsSource);
  
    // Create the shader program
    const shaderProgram = gl.createProgram();
    gl.attachShader(shaderProgram, vertexShader);
    gl.attachShader(shaderProgram, fragmentShader);
    gl.linkProgram(shaderProgram);
  
    // If creating the shader program failed, alert
    if (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) {
        alert(`Unable to initialize the shader program: ${gl.getProgramInfoLog(shaderProgram)}`);
        return null;
    }
  
    return shaderProgram;
}

//
// creates a shader of the given type, uploads the source and
// compiles it.
//
function loadShader(gl, type, source) {
    const shader = gl.createShader(type);
  
    // Send the source to the shader object
    gl.shaderSource(shader, source);
  
    // Compile the shader program
    gl.compileShader(shader);
  
    // See if it compiled successfully
    if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
        alert(`An error occurred compiling the shaders: ${gl.getShaderInfoLog(shader)}`);
        gl.deleteShader(shader);
        return null;
    }
  
    return shader;
}
```

To use this code we call it like this

```html
const shaderProgram = initShaderProgram(gl, vsSource, fsSource);
```

After we've created a shader program we need to look up the locations that WebGL assigned to our inputs. In this case we 
have one attribute and two uniforms. Attributes receive values from buffers. Each iteration of the vertex shader
receives the next value from the buffer assigned to that attribute. Uniforms are similar to JavaScript global variables. 
They stay the same value for all iterations of a shader. Since the attribute and uniform locations are specific to a 
single shader program we'll store them together to make them easy to pass around

```html
    const programInfo = {
        program: shaderProgram,
        attribLocations: {
            vertexPosition: gl.getAttribLocation(shaderProgram, 'aVertexPosition'),
        },
        uniformLocations: {
            projectionMatrix: gl.getUniformLocation(shaderProgram, 'uProjectionMatrix'),
            modelViewMatrix: gl.getUniformLocation(shaderProgram, 'uModelViewMatrix'),
        },
    };
```

#### Creating the square plane

Before we can render our square plane, we need to create the buffer that contains its vertex positions and put the
vertex positions in it. We'll do that using a function we call initBuffers(); as we explore more advanced WebGL
concepts, this routine will be augmented to create more - and more complex - 3D objects.

```html
function initBuffers(gl) {
    // Create a buffer for the square's positions.
    const positionBuffer = gl.createBuffer();
  
    // Select the positionBuffer as the one to apply buffer
    // operations to from here out.
    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
  
    // Now create an array of positions for the square.
    const positions = [
       1.0,  1.0,
      -1.0,  1.0,
       1.0, -1.0,
      -1.0, -1.0,
    ];
  
    // Now pass the list of positions into WebGL to build the
    // shape. We do this by creating a Float32Array from the
    // JavaScript array, then use it to fill the current buffer.
    gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);
  
    return {
        position: positionBuffer,
    };
}
```


#### Rendering the Scene

Once the shaders are established, the locations are looked up, and the square plane's vertex positions put in a buffer,
we can actually render the scene. Since we're not animating anything in this example, our `drawScene()` function is very 
simple. It uses a few utility routines


#### Creating 3D Objects using WebGL

Let's take our square plane into three dimensions by adding five more faces to create a cube. To do this efficiently, 
we're going to switch from drawing using the vertices directly by calling the
[gl.drawArrays()](https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/drawArrays) method to using the 
vertex array as a table, and referencing individual vertices in that table to define the positions of each face's
vertices, by calling
[gl.drawElements()](https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/drawElements).

> Each face requires four vertices to define it, but each vertex is shared by three faces. We can pass a lot fewer data 
> around by building an array of all 24 vertices, then referring to each vertex by its index into that array instead of 
> moving entire sets of coordinates around. If you wonder why we need 24 vertices, and not just 8, it is because each 
> corner belongs to three faces of different colors, and a single vertex needs to have a single specific color;
> therefore we will create three copies of each vertex in three different colors, one for each face.

##### Define the Positions of the Cube's Vertices

First, let's build the cube's vertex position buffer by updating the code in initBuffers(). This is pretty much the same 
as it was for the square plane, but somewhat longer since there are 24 vertices (4 per side):

```html
const positions = [
    // Front face
    -1.0, -1.0,  1.0,
     1.0, -1.0,  1.0,
     1.0,  1.0,  1.0,
    -1.0,  1.0,  1.0,
  
    // Back face
    -1.0, -1.0, -1.0,
    -1.0,  1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0, -1.0, -1.0,
  
    // Top face
    -1.0,  1.0, -1.0,
    -1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0, -1.0,
  
    // Bottom face
    -1.0, -1.0, -1.0,
     1.0, -1.0, -1.0,
     1.0, -1.0,  1.0,
    -1.0, -1.0,  1.0,
  
    // Right face
     1.0, -1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0,  1.0,  1.0,
     1.0, -1.0,  1.0,
  
    // Left face
    -1.0, -1.0, -1.0,
    -1.0, -1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0,  1.0, -1.0,
];
```

Since we've added a z-component to our vertices, we need to update the `numComponents` of our `vertexPosition` attribute 
to 3.

```html
// Tell WebGL how to pull out the positions from the position
// buffer into the vertexPosition attribute
{
    const numComponents = 3;
    // â€¦
    gl.vertexAttribPointer(
        programInfo.attribLocations.vertexPosition,
        numComponents,
        type,
        normalize,
        stride,
        offset
    );
    gl.enableVertexAttribArray(programInfo.attribLocations.vertexPosition);
}
```

##### Define the Vertices' Colors

We also need to build an array of colors for each of the 24 vertices. This code starts by defining a color for each
face, then uses a loop to assemble an array of all the colors for each of the vertices.

```html
const faceColors = [
    [1.0,  1.0,  1.0,  1.0],    // Front face: white
    [1.0,  0.0,  0.0,  1.0],    // Back face: red
    [0.0,  1.0,  0.0,  1.0],    // Top face: green
    [0.0,  0.0,  1.0,  1.0],    // Bottom face: blue
    [1.0,  1.0,  0.0,  1.0],    // Right face: yellow
    [1.0,  0.0,  1.0,  1.0],    // Left face: purple
];

// Convert the array of colors into a table for all the vertices.

const colors = [];

for (const c of faceColors) 
    // Repeat each color four times for the four vertices of the face
    colors.push(c, c, c, c);
}

const colorBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, colorBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(colors), gl.STATIC_DRAW);
```

##### Define the Element Array

Once the vertex arrays are generated, we need to build the element array.

```html
const indexBuffer = gl.createBuffer();
gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, indexBuffer);

// This array defines each face as two triangles, using the
// indices into the vertex array to specify each triangle's
// position.

const indices = [
    0,  1,  2,      0,  2,  3,    // front
    4,  5,  6,      4,  6,  7,    // back
    8,  9,  10,     8,  10, 11,   // top
    12, 13, 14,     12, 14, 15,   // bottom
    16, 17, 18,     16, 18, 19,   // right
    20, 21, 22,     20, 22, 23,   // left
];

// Now send the element array to GL

gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(indices), gl.STATIC_DRAW);

return {
    position: positionBuffer,
    color: colorBuffer,
    indices: indexBuffer,
};
```

> ðŸ“‹ The indices array defines each face like a pair of triangles, specifying each triangle's vertices as an index into 
> the cube's vertex arrays. Thus the cube is described as a collection of 12 triangles.

##### Drawing the Cube

Next we need to add code to our drawScene() function to draw using the cube's index buffer, adding new
[gl.bindBuffer()](https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/bindBuffer) and
[gl.drawElements()](https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/drawElements) calls:

```html
    // Tell WebGL which indices to use to index the vertices
    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, buffers.indices);
  
    // â€¦
  
    {
        const vertexCount = 36;
        const type = gl.UNSIGNED_SHORT;
        const offset = 0;
        gl.drawElements(gl.TRIANGLES, vertexCount, type, offset);
    }
```

Since each face of our cube is comprised of two triangles, there are 6 vertices per side, or 36 total vertices in the 
cube, even though many of them are duplicates.
