---
layout: post
title: node2vec - Scalable Feature Learning for Networks
tags: [Machine Learning, SNAP, Knowledge Graph, node2vec]
color: rgb(115, 167, 67)
feature-img: "assets/img/post-cover/3-cover.png"
thumbnail: "assets/img/post-cover/3-cover.png"
authors: [node2vec, skip-gram-model]
excerpt_separator: <!--more-->
---

node2vec is an algorithmic framework for representational learning on graphs. Given any graph, it can learn continuous 
feature representations for the nodes, which can then be used for various downstream machine learning tasks.

<!--more-->

* TOC
{:toc}

Motivation
----------

_Learning useful representations_ from highly structured objects such as graphs is useful for a variety of machine
learning applications. Besides reducing the engineering effort, these representations can lead to greater predictive
power. The **node2vec** framework learns low-dimensional representations for nodes in a graph by optimizing a
neighborhood preserving objective. The objective is flexible, and the algorithm accomodates for various definitions of 
network neighborhoods by simulating biased random walks. Specifically, it provides a way of balancing the 
exploration-exploitation tradeoff that in turn leads to representations obeying a spectrum of equivalences from
homophily to structural equivalence.

![Error loading node2vec-walk.png]({{ "/assets/img/node2vec-walk.png" | relative_url}})

After transitioning to node $$\mathit{v}$$ from $$\mathit{t}$$, the return hyperparameter, $$\mathit{p}$$ and the inout 
hyperparameter, $$\mathit{q}$$ control the probability of a walk staying inward revisiting nodes ($$\mathit{t}$$),
staying close to the preceeding nodes ($$\mathit{x_1}$$), or moving outward farther away ($$\mathit{x_2}$$,
$$\mathit{x_3}$$).

For example, the graph visualization below depicts the color-coded communities exhibiting homophily discovered by
_node2vec_ in the Les Misérables Network.

![Error loading node2vec-homo.png]({{ "/assets/img/node2vec-homo.png" | relative_url}})

Original Paper
--------------

[node2vec: Scalable Feature Learning for Networks](http://arxiv.org/abs/1607.00653)
([PDF]({{ "/assets/pdf/node2vec.pdf" | relative_url}})). A. Grover, J. Leskovec. _ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD)_, 2016.

### The Skip-Gram Model

The skip-gram neural network model is actually surprisingly simple in its most basic form. Let's start with a simple
example.

We're going to train a neural network to do the following. Given a specific word (the input word) in the middle of a
sentence, look at the words nearby and pick one at random. The network is going to tell us the probability for every
word in our vocabulary of being the "nearby word" that we chose.

> When I say "nearby", there is actually a "window size" parameter to the algorithm. A typical window size might be 5,
> meaning 5 words behind and 5 words ahead (10 in total).

The output probabilities are going to relate to how likely it is to find each vocabulary word nearby our input word. For
example, if we give the trained network the input word "Soviet", the output probabilities are going to be much higher
for words like "Union" and "Russia" than for unrelated words like "watermelon" and "kangaroo".

We’ll train the neural network to do this by feeding it "word pairs" found in our training documents. The example below
shows some of the training samples (word pairs) we would take from the sentence "The quick brown fox jumps over the lazy
dog." I've used a small window size of 2 just for the example. The word highlighted in blue is the input word.

![Error loading skip-diagram-training-data.png]({{ "/assets/img/skip-diagram-training-data.png" | relative_url}})

The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the
network is probably going to get many more training samples of ("Soviet", "Union") than it is of
("Soviet", "Sasquatch"). When the training is finished, if we give it the word "Soviet" as input, then it will output a
much higher probability for "Union" or "Russia" than it will for "Sasquatch".

#### The Model

We know we can't feed a word just as a text string to a neural network, so we need a way to represent the words to the 
network. To do this, we first build a vocabulary of words from our training documents - let's say we have a vocabulary
of 10,000 unique words.

We're going to represent an input word like "ants" as a
[one-hot vector](https://en.wikipedia.org/wiki/One-hot#:~:text=In%20natural%20language%20processing%2C%20a,uniquely%20to%20identify%20the%20word.).
This vector will have 10,000 components (one for every word in our vocabulary) and we'll place a "1" in the position 
corresponding to the word "ants", and 0's in all of the other positions.

The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary,
the probability that a randomly selected nearby word is that vocabulary word.

Here's the architecture of our neural network.

![Error loading skip-gram-net-arch.png]({{ "/assets/img/skip-gram-net-arch.png" | relative_url}})

There is no activation function on the hidden layer neurons, but the output neurons use _softmax_.

When training this network on word pairs, the input is a one-hot vector representing the input word and the training
output is also a one-hot vector representing the output word. For example:

![Error loading matrix-mult-w-one-hot.png]({{ "/assets/img/matrix-mult-w-one-hot.png" | relative_url}})

But when you evaluate the trained network on an input word, the output vector will actually be a probability
distribution (i.e., a bunch of floating point values, not a one-hot vector).

For our example, we're going to say that we're learning word vectors with 300 features. So the hidden layer is going to
be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every 
hidden neuron).

> 300 features is what Google used in their published model trained on the Google news dataset (you can download it from 
> [here](https://code.google.com/archive/p/word2vec/)). The number of features is a "hyper parameter" that we would just 
> have to tune to our application (that is, try different values and see what yields the best results).

The `1 x 300` word vector for "ants" then gets fed to the output layer. The output layer is a softmax regression 
classifier. There's an in-depth tutorial on Softmax Regression
[here](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/), but the gist of it is that each output neuron 
(one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will
add up to 1.

Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, 
then it applies the function `exp(x)` to the result. Finally, in order to get the outputs to sum up to 1, we divide this 
result by the sum of the results from all 10,000 output nodes.

Here's an illustration of calculating the output of the output neuron for the word "car".

![Error loading output-weights-function.png]({{ "/assets/img/output-weights-function.png" | relative_url}})

#### Implications to word2vec - Wrod Context

When we say two words have similar **contexts**, we could expect that synonyms like "intelligent" and "smart" would have 
very similar contexts. Or that words that are related, like "engine" and "transmission", would probably have similar 
contexts as well.

This can also handle stemming for us - the network will likely learn similar word vectors for the words "ant" and "ants" 
because these should have similar contexts.
